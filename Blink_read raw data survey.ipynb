{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5c83f4f-18b5-411c-80c9-e280a094e6a3",
   "metadata": {},
   "source": [
    "# Blink Project: Automated Data Processing Notebook\n",
    "\n",
    "This notebook uses a two-run workflow to transform raw survey data files (`.var`, `.dat`, `.o`) into a clean, analysis-ready dataset.\n",
    "\n",
    "## Workflow Instructions\n",
    "\n",
    "### Run 1: Setup Mode\n",
    "1.  Place your raw `.var`, `.dat`, and `.o` files in a folder.\n",
    "2.  In the final code cell, set `DATA_DIRECTORY` to that folder's path and `ID_COLUMN_NAME` to your survey's unique ID.\n",
    "3.  Set `RUN_MODE = 'SETUP'`.\n",
    "4.  Run all cells in the notebook.\n",
    "5.  The script will generate **one single Excel file** in your data directory: **`config_and_templates.xlsx`**.\n",
    "\n",
    "### Configuration Step (User's Task)\n",
    "Open the `config_and_templates.xlsx` file and edit the sheets as needed:\n",
    "\n",
    "1.  **`QuestionList` Sheet**:\n",
    "    * In the **`Keep`** column, type `yes` for variables you want in the final dataset.\n",
    "    * In the **`ChartName`** column, provide a user-friendly name for reporting.\n",
    "2.  **`LabelMap` Sheet**: Use the `CodeList` sheet as a reference to add any label revisions (`VariableName`, `OriginalLabel`, `RevisedLabel`).\n",
    "3.  **`RespondentFilter` Sheet**: (Optional) Paste a list of respondent IDs (matching the `ID_COLUMN_NAME`) that you want to **keep**. If this sheet is left blank, all respondents will be included.\n",
    "4.  **`ExtractList` Sheet**: (Optional) List the final `ChartName`s you want to export for a quick review.\n",
    "5.  **`Corrections` Sheet**: (Optional) Add any row-specific data corrections.\n",
    "\n",
    "### Run 2: Process Mode\n",
    "1.  In the final code cell, change `RUN_MODE = 'PROCESS'`.\n",
    "2.  Run all the notebook cells again.\n",
    "3.  The script will use your single, configured Excel file to generate the final outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7906da2a-b7a7-4f64-9909-9f972f89ad1b",
   "metadata": {},
   "source": [
    "## Imports and Helper Functions\n",
    "\n",
    "This cell contains utility functions for file handling, configuration, and Excel formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0ab014-5269-4be5-95e1-d6748ff1357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# Suppress the specific FutureWarning from pandas to keep the output clean\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# This class will hold the information for each variable\n",
    "@dataclass\n",
    "class VarDefinition:\n",
    "    name: str\n",
    "    type: str\n",
    "    position: int\n",
    "    length: int\n",
    "    question_text: str\n",
    "    labels: Dict[str, str] = field(default_factory=dict)\n",
    "\n",
    "def get_file_paths(directory: str) -> dict:\n",
    "    \"\"\"Scans a directory and finds the first .var, .dat, and .o files.\"\"\"\n",
    "    paths = {'var': None, 'dat': None, 'o': None}\n",
    "    print(f\"Scanning directory: {directory}\")\n",
    "    try:\n",
    "        for filename in os.listdir(directory):\n",
    "            filepath = os.path.join(directory, filename)\n",
    "            if not paths['var'] and filename.lower().endswith('.var'): paths['var'] = filepath\n",
    "            elif not paths['dat'] and filename.lower().endswith('.dat'): paths['dat'] = filepath\n",
    "            elif not paths['o'] and filename.lower().endswith('.o'): paths['o'] = filepath\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ Error: Directory not found at '{directory}'\"); return None\n",
    "    return paths\n",
    "\n",
    "def _format_excel_sheet(writer: pd.ExcelWriter, df: pd.DataFrame, sheet_name: str):\n",
    "    \"\"\"Internal helper to format a single sheet within an ExcelWriter object.\"\"\"\n",
    "    df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets[sheet_name]\n",
    "    header_format = workbook.add_format({'bold': True, 'bg_color': '#D3D3D3'})\n",
    "    \n",
    "    for col_num, value in enumerate(df.columns.values):\n",
    "        worksheet.write(0, col_num, value, header_format)\n",
    "    \n",
    "    for i, col in enumerate(df.columns):\n",
    "        column_len = len(col)\n",
    "        if not df[col].empty:\n",
    "            max_len = df[col].astype(str).str.len().max()\n",
    "            column_len = max(column_len, max_len)\n",
    "        worksheet.set_column(i, i, min(column_len + 2, 60))\n",
    "\n",
    "def save_df_to_formatted_excel(df: pd.DataFrame, path: str):\n",
    "    \"\"\"Saves a single DataFrame to a nicely formatted Excel file.\"\"\"\n",
    "    try:\n",
    "        with pd.ExcelWriter(path, engine='xlsxwriter') as writer:\n",
    "            _format_excel_sheet(writer, df, 'Sheet1')\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Warning: Could not write formatted Excel file '{os.path.basename(path)}'. Error: {e}\")\n",
    "\n",
    "def load_config_sheet(config_path: str, sheet_name: str) -> pd.DataFrame:\n",
    "    \"\"\"Loads a specific sheet from the config file, returning an empty DataFrame if not found.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_excel(config_path, sheet_name=sheet_name)\n",
    "        return df.dropna(how='all')\n",
    "    except FileNotFoundError:\n",
    "        return None\n",
    "    except ValueError:\n",
    "        print(f\"ⓘ Sheet '{sheet_name}' not found in config file. Skipping this step.\")\n",
    "        return pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"  -> Warning: Could not read sheet '{sheet_name}'. Error: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd1b8b0-0b41-4e3e-b943-c6fddf5a3b3f",
   "metadata": {},
   "source": [
    "## Parse Variables and Create Config Files\n",
    "\n",
    "This is the most critical step. The `.var` file acts as a \"codebook\" or \"map\" that defines the structure of the main data file. This function reads and interprets that map.\n",
    "\n",
    "**File Format:**\n",
    "\n",
    "The script understands four types of lines in the UTF-16 encoded `.var` file:\n",
    "\n",
    "- Variable Definition Lines: Always start with an asterisk (`*`) and define the variable's name, type (e.g., `*SNG`, `*NUM`), position, and length.\n",
    "\n",
    "- Code Frame/Label Lines: Start with a number (the code), followed by a colon, and then the text label (e.g., `1: Yes`).\n",
    "\n",
    "- Multiline Question Text: Lines starting with several spaces are treated as a continuation of the previous variable's question text.\n",
    "\n",
    "- Special `QTYPE:OPEN` Lines: This line overrides the preceding variable's type to `OPN` (Open-End).\n",
    "\n",
    "Additionaly, the script will generate the `config_and_templates.xlsx` file for the users to input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b87a96-ccde-41da-b0a0-5e4071a4311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_var_file(filepath: str) -> List[VarDefinition]:\n",
    "    \"\"\"Reads a .var file and returns a list of VarDefinition objects.\"\"\"\n",
    "    print(f\"⚙️ Parsing `{os.path.basename(filepath)}`...\")\n",
    "    # (This function is correct and unchanged)\n",
    "    var_definitions: List[VarDefinition] = []\n",
    "    current_variable: VarDefinition = None\n",
    "    with open(filepath, 'r', encoding='utf-16') as f:\n",
    "        for line in f:\n",
    "            line_stripped = line.strip()\n",
    "            if not line_stripped: continue\n",
    "            if line_stripped.startswith('*'):\n",
    "                try:\n",
    "                    definition_part, question_text = line_stripped.split(':', 1)\n",
    "                    parts = definition_part.strip().split()\n",
    "                    pos_len_part = parts[-1]\n",
    "                    position, length = map(int, pos_len_part.split('L'))\n",
    "                    if len(parts) >= 3: var_name, var_type = parts[0][1:], parts[1][1:]\n",
    "                    else: var_name, var_type = parts[0][1:], \"NUM\"\n",
    "                    current_variable = VarDefinition(name=var_name, type=var_type, position=position, length=length, question_text=question_text.strip())\n",
    "                    var_definitions.append(current_variable)\n",
    "                except (ValueError, IndexError):\n",
    "                    print(f\"  Warning: Could not parse variable line: {line_stripped}\")\n",
    "                    current_variable = None\n",
    "            elif \"QTYPE:OPEN\" in line_stripped:\n",
    "                if current_variable: current_variable.type = 'OPN'\n",
    "            elif ':' in line_stripped and line_stripped.split(':', 1)[0].strip().isdigit():\n",
    "                if current_variable:\n",
    "                    try:\n",
    "                        code, label = line_stripped.split(':', 1)\n",
    "                        current_variable.labels[code.strip()] = label.strip()\n",
    "                    except ValueError:\n",
    "                        print(f\"  Warning: Could not parse label line: {line_stripped}\")\n",
    "            elif line.startswith('   ') and current_variable:\n",
    "                current_variable.question_text += \" \" + line_stripped\n",
    "    return var_definitions\n",
    "\n",
    "def export_setup_files(parsed_vars: List[VarDefinition], output_path: str, id_column_name: str, filenames: dict, dat_filepath: str):\n",
    "    \"\"\"Exports a single, multi-sheet Excel file with all templates, ensuring RespondentList has numeric IDs.\"\"\"\n",
    "    print(f\"⚙️ Exporting setup files to a single workbook: `{os.path.basename(output_path)}`...\")\n",
    "    try:\n",
    "        all_respondent_ids = []\n",
    "        if dat_filepath:\n",
    "            print(\"  -> Reading respondent IDs from .dat file for the list...\")\n",
    "            id_var = next((var for var in parsed_vars if var.name == id_column_name), None)\n",
    "            if id_var:\n",
    "                colspec = [(id_var.position - 1, id_var.position - 1 + id_var.length)]\n",
    "                df_ids = pd.read_fwf(dat_filepath, colspecs=colspec, header=None, encoding='utf-16', dtype=str)\n",
    "                # Convert IDs to a clean, numeric list for the template\n",
    "                all_respondent_ids = pd.to_numeric(df_ids.iloc[:, 0], errors='coerce').dropna().astype(int).unique().tolist()\n",
    "        \n",
    "        with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
    "            sheets_to_create = {\n",
    "                filenames['qlist']: pd.DataFrame([{'VariableName': v.name, 'Type': v.type, 'QuestionText': v.question_text, 'Keep': '', 'ChartName': ''} for v in parsed_vars]),\n",
    "                filenames['codelist']: pd.DataFrame([{'VariableName': v.name, 'Type': v.type, 'QuestionText': v.question_text, 'LabelCode': c, 'OriginalLabel': l} for v in parsed_vars if v.labels for c, l in v.labels.items()]),\n",
    "                filenames['label_map']: pd.DataFrame(columns=['VariableName', 'OriginalLabel', 'RevisedLabel']),\n",
    "                filenames['respondent_list']: pd.DataFrame({id_column_name: all_respondent_ids}),\n",
    "                filenames['respondent_filter']: pd.DataFrame(columns=[id_column_name]),\n",
    "                filenames['extract_list']: pd.DataFrame(columns=['ChartName']),\n",
    "                filenames['corrections']: pd.DataFrame(columns=[id_column_name, 'VariableName', 'NewValue'])\n",
    "            }\n",
    "            for sheet_name, df in sheets_to_create.items():\n",
    "                _format_excel_sheet(writer, df, sheet_name)\n",
    "    except Exception as e:\n",
    "        print(f\"  -> ERROR: Could not write the setup file. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f9411a-e87d-49c2-acaf-891a5e7b1abf",
   "metadata": {},
   "source": [
    "## Build the Final Label Map\n",
    "\n",
    "This function prepares all the labels upfront. It takes the original labels from the `.var` file and applies your revisions from the LabelMap sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2887a230-15c7-4617-941f-e75f6a42eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_final_label_map(var_definitions: List[VarDefinition], revision_map: pd.DataFrame) -> dict:\n",
    "    \"\"\"Creates a final, revised master map of all labels *before* any data transformation.\"\"\"\n",
    "    print(\"⚙️ Building final label map with user revisions...\")\n",
    "    \n",
    "    # Start with the original labels from the .var file\n",
    "    master_label_map = {var.name: var.labels.copy() for var in var_definitions if var.labels}\n",
    "    \n",
    "    # Apply revisions from the user's configuration file\n",
    "    if not revision_map.empty:\n",
    "        for _, row in revision_map.iterrows():\n",
    "            var_name = row['VariableName']\n",
    "            original_label = row['OriginalLabel']\n",
    "            revised_label = row['RevisedLabel']\n",
    "            \n",
    "            # Find the code corresponding to the original label to update the map\n",
    "            if var_name in master_label_map:\n",
    "                for code, label in master_label_map[var_name].items():\n",
    "                    if label == original_label:\n",
    "                        master_label_map[var_name][code] = revised_label\n",
    "                        break # Move to the next revision\n",
    "        print(f\"  -> Successfully applied {len(revision_map)} label revisions to the master map.\")\n",
    "    else:\n",
    "        print(\"  -> `LabelMap` sheet is empty or not found. Using original labels only.\")\n",
    "        \n",
    "    return master_label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42822666-0024-425f-b89b-bc5f0af9abc4",
   "metadata": {},
   "source": [
    "## Read and Process the Coded Data File (`.dat`)\n",
    "\n",
    "These functions use the map created in Step 1 to read and transform the raw respondent data from the .dat file.\n",
    "\n",
    "**2.1 read_dat_file**\n",
    "This function reads the raw data. It constructs lists of column positions (`colspecs`) and column `names` from the parsed variable definitions. It then uses `pandas.read_fwf` to efficiently parse the fixed-width text file, ensuring `encoding='utf-16'` is used. All data is read as text (`dtype=str`) initially to prevent data type errors.\n",
    "\n",
    "**2.2 filter_by_respondent_id**\n",
    "This function filters the main DataFrame to only include respondent IDs from a given list\n",
    "\n",
    "**2.3 process_multi_variables**\n",
    "This function handles the special logic for Multi-Value (`MV`) variables. An MV variable is a string like `'10100'` where each character's position represents an answer option. The function expands this single column into multiple new columns (e.g., `Q2_1`, `Q2_2`, etc.). For each new column, it checks the corresponding character in the original string: if it's `'1'`, the cell gets the text label; otherwise, it's left blank.\n",
    "\n",
    "**2.4 translate_codes_to_labels**\n",
    "This funtion translates all codes to their original labels from the .var file, then applies if revisions from the label map file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a36e8bb-d0c0-41d7-95ba-f7665bf6afa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dat_file(dat_filepath: str, var_definitions: List[VarDefinition], id_column: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads a fixed-width .dat file and immediately converts the ID column to a numeric type.\"\"\"\n",
    "    colspecs, names = [], []\n",
    "    for var in var_definitions:\n",
    "        if var.type:\n",
    "            start_pos, end_pos = var.position - 1, var.position - 1 + var.length\n",
    "            colspecs.append((start_pos, end_pos))\n",
    "            names.append(var.name)\n",
    "    print(f\"⚙️ Reading `{os.path.basename(dat_filepath)}`...\")\n",
    "    df = pd.read_fwf(dat_filepath, colspecs=colspecs, header=None, names=names, encoding='utf-16', dtype=str)\n",
    "    \n",
    "    print(f\"  -> Converting ID column '{id_column}' to a numeric type.\")\n",
    "    df[id_column] = pd.to_numeric(df[id_column], errors='coerce')\n",
    "    return df\n",
    "\n",
    "def filter_by_respondent_id(dataframe: pd.DataFrame, respondent_list_df: pd.DataFrame, id_column: str) -> pd.DataFrame:\n",
    "    \"\"\"Filters the main DataFrame to only include respondent IDs from a given list.\"\"\"\n",
    "    print(\"⚙️ Filtering by respondent ID list...\")\n",
    "    if respondent_list_df.empty:\n",
    "        print(\"  -> Respondent filter list is empty. Keeping all respondents.\"); return dataframe\n",
    "    \n",
    "    # Convert IDs from the Excel file to a numeric type for a reliable match\n",
    "    ids_to_keep = pd.to_numeric(respondent_list_df.iloc[:, 0], errors='coerce').dropna()\n",
    "    \n",
    "    original_rows = len(dataframe)\n",
    "    df_filtered = dataframe[dataframe[id_column].isin(ids_to_keep)].copy()\n",
    "    print(f\"  -> Kept {len(df_filtered)} of {original_rows} respondents based on the filter list.\")\n",
    "    return df_filtered\n",
    "\n",
    "def process_multi_variables(dataframe: pd.DataFrame, var_definitions: List[VarDefinition], master_label_map: dict) -> pd.DataFrame:\n",
    "    \"\"\"Expands MV variables using the final, revised master label map.\"\"\"\n",
    "    print(\"⚙️ Processing Multi-Value (MV) variables...\")\n",
    "    # (The logic for expanding columns is the same, but it now uses the master_label_map for its values)\n",
    "    original_cols = dataframe.columns.tolist()\n",
    "    new_column_dfs = []\n",
    "    mv_vars = [var for var in var_definitions if var.type == 'MV']\n",
    "    \n",
    "    for var in mv_vars:\n",
    "        if var.name not in original_cols: continue\n",
    "        \n",
    "        final_labels = master_label_map.get(var.name, {})\n",
    "        if not final_labels: continue\n",
    "        \n",
    "        new_cols_dict = {}\n",
    "        for code, label in final_labels.items():\n",
    "            new_col_name = f\"{var.name}_{code}\"\n",
    "            try:\n",
    "                char_index = int(code) - 1\n",
    "                if char_index < 0: continue\n",
    "                # THE FIX: It now uses the potentially revised 'label' from the master map\n",
    "                new_cols_dict[new_col_name] = dataframe[var.name].str[char_index].apply(lambda char: label if char == '1' else '')\n",
    "            except (IndexError, ValueError):\n",
    "                print(f\"  Warning: Could not process code '{code}' for variable '{var.name}'.\")\n",
    "                continue\n",
    "        if new_cols_dict:\n",
    "            new_column_dfs.append(pd.DataFrame(new_cols_dict))\n",
    "        original_cols.remove(var.name)\n",
    "        \n",
    "    return pd.concat([dataframe[original_cols]] + new_column_dfs, axis=1)\n",
    "\n",
    "def translate_codes_to_labels(dataframe: pd.DataFrame, var_definitions: List[VarDefinition], master_label_map: dict) -> pd.DataFrame:\n",
    "    \"\"\"Translates codes to labels for SNG variables using the final master label map.\"\"\"\n",
    "    print(\"⚙️ Translating codes to labels for Single-Value (SNG) variables...\")\n",
    "    \n",
    "    # THE FIX: This function is now much simpler. It only handles SNG variables.\n",
    "    sng_vars = {var.name for var in var_definitions if var.type == 'SNG'}\n",
    "    \n",
    "    for col_name in sng_vars:\n",
    "        if col_name in dataframe.columns and col_name in master_label_map:\n",
    "            # Replace codes with the final, revised labels from the master map\n",
    "            dataframe[col_name] = dataframe[col_name].str.strip().replace(master_label_map[col_name])\n",
    "            \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc57ec9-5741-474e-9fa3-8dc775c91a48",
   "metadata": {},
   "source": [
    "## Process and Merge the Open-Ended Data (`.o`) File\n",
    "\n",
    "This is the final data integration step, where verbatim text answers are added to the dataset.\n",
    "\n",
    "**4a. parse_o_file**\n",
    "The `.o` file contains both standard and \"extended\" formats for its lines. This function reads the file line by line (not with `read_fwf`) to handle this complexity. It checks each line for the extended format marker (`*`) to determine the correct character positions for the respondent ID, the question's original position, and the answer text. It returns a clean DataFrame of the parsed open-ended data.\n",
    "\n",
    "**4b. merge_o_data**\n",
    "This function merges the parsed open-ended data into the main DataFrame. It first creates a quick lookup map to get a variable's name from its position (`pos_to_name_map`). It then iterates through each open-ended answer, using the respondent ID and the mapped variable name to find the exact cell (`.loc`) in the main DataFrame to place the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19221590-1d87-4220-8f08-2091f1d70bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_o_file(o_filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Reads the .o file line-by-line to handle standard and extended formats.\"\"\"\n",
    "    print(f\"⚙️ Parsing `{os.path.basename(o_filepath)}` for open-ended answers...\")\n",
    "    parsed_records = []\n",
    "    try:\n",
    "        with open(o_filepath, 'r', encoding='utf-16') as f:\n",
    "            for line in f:\n",
    "                if len(line) < 22: continue\n",
    "                respondent_id, answer_text, question_pos_str = line[0:8].strip(), \"\", \"\"\n",
    "                if line[10:11] == '*':\n",
    "                    question_pos_str, answer_text = line[11:21].strip(), line[31:].strip()\n",
    "                else:\n",
    "                    question_pos_str, answer_text = line[10:15].strip(), line[18:].strip()\n",
    "                if question_pos_str.isdigit():\n",
    "                    parsed_records.append({'id': respondent_id, 'position': int(question_pos_str), 'text': answer_text})\n",
    "    except FileNotFoundError:\n",
    "        print(\"  -> .o file not found, skipping.\"); return pd.DataFrame()\n",
    "    return pd.DataFrame(parsed_records)\n",
    "\n",
    "def merge_o_data(main_df: pd.DataFrame, df_o: pd.DataFrame, var_definitions: List[VarDefinition], id_column: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merges the parsed open-ended data into the main DataFrame using a robust\n",
    "    and simple update method.\n",
    "    \"\"\"\n",
    "    if df_o.empty: return main_df\n",
    "    print(\"⚙️ Merging open-ended data into main table...\")\n",
    "    \n",
    "    # 1. Create a map to convert question position to variable name\n",
    "    pos_to_name_map = {var.position: var.name for var in var_definitions}\n",
    "    \n",
    "    # 2. Prepare the open-ended data\n",
    "    df_o['id'] = pd.to_numeric(df_o['id'], errors='coerce')\n",
    "    df_o['VariableName'] = df_o['position'].map(pos_to_name_map)\n",
    "    df_o.dropna(subset=['id', 'VariableName'], inplace=True)\n",
    "    \n",
    "    # 3. Pivot the data so there is one row per respondent, with columns for each OPN variable\n",
    "    df_o_pivoted = df_o.pivot(index='id', columns='VariableName', values='text')\n",
    "    \n",
    "    # 4. THE FIX: Use a robust 'update' method\n",
    "    # This aligns the data on the respondent ID and fills in the blanks in the main\n",
    "    # DataFrame with the text from the open-ended data, without dropping any columns.\n",
    "    main_df_indexed = main_df.set_index(id_column)\n",
    "    main_df_indexed.update(df_o_pivoted)\n",
    "    main_df = main_df_indexed.reset_index()\n",
    "    \n",
    "    print(f\"  -> Successfully merged open-ended answers.\")\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf265315-6d3c-41de-8243-a37bf216cdc0",
   "metadata": {},
   "source": [
    "## Apply Manual Corrections (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb35940-2b7d-483b-bc28-b9460b316220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_manual_corrections(dataframe: pd.DataFrame, corrections_df: pd.DataFrame, id_column: str) -> pd.DataFrame:\n",
    "    \"\"\"Applies manual data corrections using numeric IDs.\"\"\"\n",
    "    print(\"⚙️ Applying manual data corrections...\")\n",
    "    if corrections_df.empty:\n",
    "        print(\"  -> Correction sheet is empty. Skipping.\"); return dataframe\n",
    "    \n",
    "    df = dataframe.copy()\n",
    "    # THE FIX: Ensure IDs from the corrections file are also numeric for matching\n",
    "    corrections_df[id_column] = pd.to_numeric(corrections_df[id_column], errors='coerce')\n",
    "\n",
    "    # Set the ID as the index for efficient updating\n",
    "    df.set_index(id_column, inplace=True)\n",
    "    \n",
    "    for _, row in corrections_df.iterrows():\n",
    "        respondent_id = row[id_column]\n",
    "        var_name = row['VariableName']\n",
    "        new_value = row['NewValue']\n",
    "        \n",
    "        # Check if the respondent and variable exist before trying to update\n",
    "        if pd.notna(respondent_id) and respondent_id in df.index and var_name in df.columns:\n",
    "            df.loc[respondent_id, var_name] = new_value\n",
    "            \n",
    "    print(f\"  -> Applied {len(corrections_df)} corrections.\")\n",
    "    return df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cc3d41-d763-494c-873e-352750f0078c",
   "metadata": {},
   "source": [
    "## Custom Transformations (optional)\n",
    "This cell is for creating new, calculated variables (derived variables) from the cleaned data. Add your project-specific transformations here *before* running the final cell. This function will be called during the `'PROCESS'` run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8707c238-f41f-44b9-bfd6-687e54969c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_derived_variables(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Creates new columns in the DataFrame based on custom calculations.\"\"\"\n",
    "    print(\"⚙️ Creating derived variables...\")\n",
    "\n",
    "    # --- Add your custom logic below ---\n",
    "\n",
    "    # Example 1: Create Age Groups from an 'AGE' column\n",
    "    if 'AGE' in df.columns:\n",
    "        age_numeric = pd.to_numeric(df['AGE'], errors='coerce')\n",
    "        bins = [0, 24, 39, 54, 100]\n",
    "        labels = ['18-24', '25-39', '40-54', '55+']\n",
    "        df['Age_Group'] = pd.cut(age_numeric, bins=bins, labels=labels, right=False)\n",
    "        print(\"  -> Created 'Age_Group' column.\")\n",
    "\n",
    "    # Example 2: Combine 'Awareness' columns into a total score\n",
    "    awareness_cols = [col for col in df.columns if 'Aware_' in col and col.endswith('_1')]\n",
    "    if awareness_cols:\n",
    "        for col in awareness_cols:\n",
    "             df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "        df['Total_Awareness'] = df[awareness_cols].sum(axis=1)\n",
    "        print(\"  -> Created 'Total_Awareness' column.\")\n",
    "        \n",
    "    # --- End of custom logic ---\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb90b13-3c1e-4dec-b87c-ca9e7338ea8d",
   "metadata": {},
   "source": [
    "## Extract Selected Data to Excel (optional)\n",
    "\n",
    "The final step is to save the results. The main script saves the complete, cleaned dataset. This helper function provides an optional way to save a smaller subset of columns to a separate Excel file for quick review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76effebe-32b4-4e3b-859b-b93726917a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data_to_excel(dataframe: pd.DataFrame, extract_list_df: pd.DataFrame, output_path: str):\n",
    "    \"\"\"\n",
    "    Exports a subset of columns to a separate, formatted Excel file for review.\n",
    "    UPDATED: Now correctly handles requests for Multi-Value (MV) variables by looking for\n",
    "    column names that start with the requested ChartName.\n",
    "    \"\"\"\n",
    "    if extract_list_df.empty or extract_list_df.iloc[:, 0].dropna().empty:\n",
    "        print(\"ⓘ Extract list is empty, skipping data extraction.\"); return\n",
    "        \n",
    "    chart_names_to_extract = extract_list_df.iloc[:, 0].dropna().tolist()\n",
    "    print(f\"⚙️ Exporting {len(chart_names_to_extract)} specified variables to `{os.path.basename(output_path)}`...\")\n",
    "    \n",
    "    final_cols_to_extract = []\n",
    "    found_chart_names = set()\n",
    "\n",
    "    # Iterate through the user's requested chart names and find all matching columns\n",
    "    for name in chart_names_to_extract:\n",
    "        # Find exact matches (for SNG, OPN, etc.)\n",
    "        if name in dataframe.columns:\n",
    "            final_cols_to_extract.append(name)\n",
    "            found_chart_names.add(name)\n",
    "        \n",
    "        # Find partial matches for expanded MV columns (e.g., \"Aided Awareness_51\")\n",
    "        mv_matches = [col for col in dataframe.columns if col.startswith(f\"{name}_\")]\n",
    "        if mv_matches:\n",
    "            final_cols_to_extract.extend(mv_matches)\n",
    "            found_chart_names.add(name)\n",
    "\n",
    "    # Report any requested names that didn't match any columns\n",
    "    missing_names = [name for name in chart_names_to_extract if name not in found_chart_names]\n",
    "    if missing_names:\n",
    "        print(f\"  -> Warning: The following ChartNames from the extract list were not found: {missing_names}\")\n",
    "    \n",
    "    if not final_cols_to_extract:\n",
    "        print(\"  -> Warning: None of the variables in the extract list could be found.\"); return\n",
    "        \n",
    "    save_df_to_formatted_excel(dataframe[final_cols_to_extract], output_path)\n",
    "    print(f\"  -> Successfully created extract file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca2ce58-d8d9-4200-9717-782ede9f07a9",
   "metadata": {},
   "source": [
    "## Generate Unpivoted MV Files\n",
    "\n",
    "This function identify which MV variables to unpivot based on user's selections in the QuestionList sheet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c409d3a8-316a-4949-b1eb-81e891ac09bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_unpivoted_mv_files(dataframe: pd.DataFrame, vars_to_keep_df: pd.DataFrame, id_column: str, rename_map: dict, output_dir: str):\n",
    "    \"\"\"\n",
    "    Automatically finds selected MV variables and creates separate, \n",
    "    unpivoted (long-format) CSV files for them.\n",
    "    \"\"\"\n",
    "    # THE FIX: Identify which MV variables the user wants to keep\n",
    "    mv_vars_to_unpivot = vars_to_keep_df[vars_to_keep_df['Type'] == 'MV']['VariableName'].tolist()\n",
    "\n",
    "    if not mv_vars_to_unpivot:\n",
    "        print(\"ⓘ No MV variables selected to unpivot. Skipping this step.\")\n",
    "        return\n",
    "        \n",
    "    print(f\"⚙️ Generating {len(mv_vars_to_unpivot)} unpivoted MV files...\")\n",
    "\n",
    "    for var_name in mv_vars_to_unpivot:\n",
    "        # Find all the expanded columns for this MV variable (e.g., 'BE2_51', 'BE2_53', ...)\n",
    "        expanded_cols = [col for col in dataframe.columns if col.startswith(f\"{var_name}_\")]\n",
    "        \n",
    "        if not expanded_cols:\n",
    "            print(f\"  -> Warning: No expanded columns found for MV variable '{var_name}'. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Get the final, user-friendly ChartName for this variable\n",
    "        chart_name = rename_map.get(var_name, var_name)\n",
    "        \n",
    "        # Use pandas 'melt' to unpivot the data\n",
    "        df_melted = dataframe.melt(\n",
    "            id_vars=[id_column],\n",
    "            value_vars=expanded_cols,\n",
    "            var_name='OriginalColumn', # Temporary column\n",
    "            value_name=chart_name      # The final column with the brand names\n",
    "        )\n",
    "        \n",
    "        # Clean up the resulting table\n",
    "        df_melted = df_melted[df_melted[chart_name] != ''].dropna()\n",
    "        df_melted = df_melted[[id_column, chart_name]] # Keep only the two required columns\n",
    "        \n",
    "        # Save to its own CSV file\n",
    "        output_filename = f\"unpivoted_{var_name}.csv\"\n",
    "        output_path = os.path.join(output_dir, output_filename)\n",
    "        df_melted.to_csv(output_path, index=False, encoding='utf-8-sig')\n",
    "        print(f\"  -> Successfully created `{output_filename}` with {len(df_melted)} rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71bbe0fe-550c-4e1e-ba8a-4f096711b45a",
   "metadata": {},
   "source": [
    "## ▶️ Run the Pipeline\n",
    "This is the main execution cell. Configure your settings below and run this cell to perform either the **SETUP** or **PROCESS** run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0498fae5-6b91-429d-bec6-6462b9cefff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. SET YOUR CONFIGURATION HERE ---\n",
    "RUN_MODE = 'PROCESS' # Input either 'SETUP' or 'PROCESS'\n",
    "DATA_DIRECTORY = r'C:\\Users\\NguyenGi\\OneDrive - Kantar\\General\\Automation\\Blink Project\\Input data\\Golden 2024\\202404-OF-L GOLDEN 2024' # Input path to data folder\n",
    "ID_COLUMN_NAME = 'INTNR' # Input name of the variable that houses the respondent unique ID, usually the first variable in QuestionList\n",
    "SAVE_FULL_OUTPUT = True\n",
    "\n",
    "# --- 2. DEFINE FILENAMES ---\n",
    "FILENAMES = {\n",
    "    'config': 'config_and_templates.xlsx',\n",
    "    'qlist': 'QuestionList',\n",
    "    'codelist': 'CodeList',\n",
    "    'label_map': 'LabelMap',\n",
    "    'respondent_list': 'RespondentList',\n",
    "    'respondent_filter': 'RespondentFilter',\n",
    "    'extract_list': 'ExtractList',\n",
    "    'corrections': 'Corrections',\n",
    "    'final_output': 'data_final_output.csv',\n",
    "    'extract_output': 'data_extract.xlsx'\n",
    "}\n",
    "\n",
    "# --- 3. RUN THE PIPELINE BASED ON THE SELECTED MODE ---\n",
    "file_paths = get_file_paths(DATA_DIRECTORY)\n",
    "config_path = os.path.join(DATA_DIRECTORY, FILENAMES['config'])\n",
    "\n",
    "if RUN_MODE.upper() == 'SETUP':\n",
    "    print(\"\\n--- Running in SETUP mode ---\")\n",
    "    if file_paths and file_paths['var']:\n",
    "        parsed_vars = parse_var_file(file_paths['var'])\n",
    "        export_setup_files(parsed_vars, config_path, ID_COLUMN_NAME, FILENAMES, file_paths.get('dat'))\n",
    "        print(f\"\\n✅ SETUP COMPLETE. Please edit the generated file: '{FILENAMES['config']}'.\")\n",
    "    else:\n",
    "        print(\"\\n❌ Critical Error: Could not find a .var file.\")\n",
    "\n",
    "elif RUN_MODE.upper() == 'PROCESS':\n",
    "    print(\"\\n--- Running in PROCESS mode ---\")\n",
    "    if file_paths and file_paths['var'] and file_paths['dat']:\n",
    "        try:\n",
    "            # --- Load All Configuration Sheets ---\n",
    "            df_qlist = load_config_sheet(config_path, FILENAMES['qlist'])\n",
    "            df_label_map = load_config_sheet(config_path, FILENAMES['label_map'])\n",
    "            df_respondent_filter = load_config_sheet(config_path, FILENAMES['respondent_filter'])\n",
    "            df_corrections = load_config_sheet(config_path, FILENAMES['corrections'])\n",
    "            df_extract_list = load_config_sheet(config_path, FILENAMES['extract_list'])\n",
    "            \n",
    "            if df_qlist is None: raise FileNotFoundError(f\"'{FILENAMES['config']}' not found.\")\n",
    "\n",
    "            # --- Run Full Pipeline ---\n",
    "            parsed_vars = parse_var_file(file_paths['var'])\n",
    "\n",
    "            # --- Build the final, revised label map FIRST ---\n",
    "            final_label_map = build_final_label_map(parsed_vars, df_label_map)\n",
    "            \n",
    "            df_main = read_dat_file(file_paths['dat'], parsed_vars, ID_COLUMN_NAME)\n",
    "            df_main = filter_by_respondent_id(df_main, df_respondent_filter, ID_COLUMN_NAME)\n",
    "            \n",
    "            # Pass the final map to the processing functions\n",
    "            df_main = process_multi_variables(df_main, parsed_vars, final_label_map)\n",
    "            df_main = translate_codes_to_labels(df_main, parsed_vars, final_label_map)\n",
    "            \n",
    "            if file_paths['o']:\n",
    "                df_o_parsed = parse_o_file(file_paths['o'])\n",
    "                if not df_o_parsed.empty:\n",
    "                    df_main = merge_o_data(df_main, df_o_parsed, parsed_vars, ID_COLUMN_NAME)\n",
    "            \n",
    "            df_main = apply_manual_corrections(df_main, df_corrections, ID_COLUMN_NAME)\n",
    "            df_main = create_derived_variables(df_main)\n",
    "            print(\"\\n✅ All data processing steps are complete.\")\n",
    "\n",
    "            # --- Filter and Rename Columns ---\n",
    "            vars_to_keep_df = df_qlist[df_qlist['Keep'].astype(str).str.lower() == 'yes']\n",
    "            user_selection = vars_to_keep_df['VariableName'].tolist()\n",
    "            rename_map = vars_to_keep_df.set_index('VariableName')['ChartName'].dropna().to_dict()\n",
    "            \n",
    "            final_cols_to_keep = []\n",
    "            if user_selection:\n",
    "                print(f\"⚙️ Selecting variables based on `{FILENAMES['qlist']}` sheet...\")\n",
    "                mv_bases = {var.name for var in parsed_vars if var.type == 'MV'}\n",
    "                for var_name in user_selection:\n",
    "                    if var_name in mv_bases:\n",
    "                        final_cols_to_keep.extend([col for col in df_main.columns if col.startswith(f\"{var_name}_\")])\n",
    "                    elif var_name in df_main.columns:\n",
    "                        final_cols_to_keep.append(var_name)\n",
    "                df_final = df_main[final_cols_to_keep].copy()\n",
    "            else:\n",
    "                print(\"ⓘ 'Keep' column is empty in config file. Keeping all variables.\")\n",
    "                df_final = df_main.copy()\n",
    "            \n",
    "            # Create a new, expanded rename map for all columns, including MV\n",
    "            expanded_rename_map = {}\n",
    "            for original_name, new_name in rename_map.items():\n",
    "                # Find all columns that start with the original MV name (e.g., \"BE2_\")\n",
    "                mv_cols_to_rename = [col for col in df_final.columns if col.startswith(f\"{original_name}_\")]\n",
    "                if mv_cols_to_rename:\n",
    "                    # Create a new name for each expanded column (e.g., \"Aided Awareness_51\")\n",
    "                    for col in mv_cols_to_rename:\n",
    "                        suffix = col.split('_', 1)[1] # Gets the part after the first underscore\n",
    "                        expanded_rename_map[col] = f\"{new_name}_{suffix}\"\n",
    "                elif original_name in df_final.columns:\n",
    "                    # It's a regular (SNG, OPN, etc.) variable\n",
    "                    expanded_rename_map[original_name] = new_name\n",
    "            \n",
    "            df_final.rename(columns=expanded_rename_map, inplace=True)\n",
    "            print(\"⚙️ Final variables selected and renamed.\")\n",
    "            \n",
    "            # --- Save Final Output(s) ---\n",
    "            if SAVE_FULL_OUTPUT:\n",
    "                final_output_path = os.path.join(DATA_DIRECTORY, FILENAMES['final_output'])\n",
    "                df_final.to_csv(final_output_path, index=False, encoding='utf-8-sig')\n",
    "                print(f\"✅ Full output saved to `{final_output_path}`\")\n",
    "            \n",
    "            extract_data_to_excel(df_final, df_extract_list, os.path.join(DATA_DIRECTORY, FILENAMES['extract_output']))\n",
    "            \n",
    "            # --- Generate Unpivoted MV Files (no change needed here) ---\n",
    "            generate_unpivoted_mv_files(df_main, vars_to_keep_df, ID_COLUMN_NAME, rename_map, DATA_DIRECTORY)\n",
    "            \n",
    "            print(\"\\n✅✅✅ Pipeline Complete! ✅✅✅\")\n",
    "            \n",
    "            print(\"\\nShowing first 5 rows of the final, processed data:\")\n",
    "            display(df_final.head())\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"An unexpected error occurred during processing: {e}\")\n",
    "    else:\n",
    "        print(\"\\n❌ Critical Error: Could not find the required .var and .dat files.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
